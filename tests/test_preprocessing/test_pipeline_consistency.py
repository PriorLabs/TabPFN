"""Check that preprocessing pipeline output matches reference transformations.

This ensures that the preprocessing behavior does not change unintentionally
during refactoring or other changes.

The transformed outputs can vary slightly between platforms, thus we maintain
separate reference data for all the platforms in `ENABLED_PLATFORMS`. The tests
are skipped on other platforms.

If the outputs change legitimately, the reference data can be regenerated by
running this file: `python -m tests.test_preprocessing.test_pipeline_consistency`.
"""

from __future__ import annotations

import json
import logging
import pathlib
import platform
from dataclasses import dataclass
from functools import partial
from typing import Callable, Literal

import numpy as np
import pytest

from tabpfn.preprocessing.configs import EnsembleConfig, PreprocessorConfig
from tabpfn.preprocessing.datamodel import ColumnMetadata, FeatureModality
from tabpfn.preprocessing.pipeline import build_pipeline

logger = logging.getLogger(__name__)


def _get_random_data_with_categoricals(
    random_state: np.random.Generator,
    n_samples: int = 30,
    n_numerical: int = 5,
    n_categorical: int = 3,
) -> tuple[np.ndarray, ColumnMetadata]:
    """Generate random data with both numerical and categorical features.

    Args:
        random_state: Random state for reproducibility.
        n_samples: Number of samples.
        n_numerical: Number of numerical features.
        n_categorical: Number of categorical features.

    Returns:
        Tuple of (X, metadata) where X is the data array and metadata
        contains column modality information.
    """
    n_features = n_numerical + n_categorical
    X = np.zeros((n_samples, n_features), dtype=np.float64)

    # Numerical features: random continuous values with some variation
    X[:, :n_numerical] = random_state.standard_normal((n_samples, n_numerical)) * 10

    # Add some NaNs to numerical features to test NaN handling
    nan_mask = random_state.random((n_samples, n_numerical)) < 0.1
    X[:, :n_numerical][nan_mask] = np.nan

    # Categorical features: random integers representing categories
    for i in range(n_numerical, n_features):
        num_categories = random_state.integers(2, 6)
        X[:, i] = random_state.integers(0, num_categories, size=n_samples).astype(float)

    # Build column metadata
    metadata = ColumnMetadata.from_dict(
        {
            FeatureModality.NUMERICAL: list(range(n_numerical)),
            FeatureModality.CATEGORICAL: list(range(n_numerical, n_features)),
        }
    )

    return X, metadata


def _create_ensemble_config(
    preprocess_config: PreprocessorConfig,
    *,
    add_fingerprint_feature: bool = False,
    polynomial_features: Literal["no", "all"] | int = "no",
    feature_shift_count: int = 0,
    feature_shift_decoder: Literal["shuffle", "rotate"] | None = None,
) -> EnsembleConfig:
    """Create an EnsembleConfig for testing.

    Args:
        preprocess_config: The preprocessor configuration.
        add_fingerprint_feature: Whether to add fingerprint features.
        polynomial_features: Polynomial features setting.
        feature_shift_count: Feature shift count.
        feature_shift_decoder: Feature shift method.

    Returns:
        EnsembleConfig instance.
    """
    return EnsembleConfig(
        preprocess_config=preprocess_config,
        add_fingerprint_feature=add_fingerprint_feature,
        polynomial_features=polynomial_features,
        feature_shift_count=feature_shift_count,
        feature_shift_decoder=feature_shift_decoder,
        subsample_ix=None,
        outlier_removal_std=None,
        _model_index=0,
    )


@dataclass(frozen=True)
class _PipelineConsistencyCase:
    """Test case for pipeline consistency testing."""

    config_factory: Callable[[], EnsembleConfig]
    description: str


# Define test cases covering various configuration options
_PREPROCESSOR_CONFIGS: dict[str, PreprocessorConfig] = {
    # Core transformations
    "none_numeric": PreprocessorConfig(
        name="none",
        categorical_name="numeric",
        max_features_per_estimator=500,
    ),
    "none_ordinal": PreprocessorConfig(
        name="none",
        categorical_name="ordinal",
        max_features_per_estimator=500,
    ),
    "none_onehot": PreprocessorConfig(
        name="none",
        categorical_name="onehot",
        max_features_per_estimator=500,
    ),
    "none_ordinal_shuffled": PreprocessorConfig(
        name="none",
        categorical_name="ordinal_shuffled",
        max_features_per_estimator=500,
    ),
    # Quantile transformations
    "quantile_uni_coarse": PreprocessorConfig(
        name="quantile_uni_coarse",
        categorical_name="numeric",
        max_features_per_estimator=500,
    ),
    "quantile_uni": PreprocessorConfig(
        name="quantile_uni",
        categorical_name="ordinal_shuffled",
        max_features_per_estimator=500,
    ),
    # Squashing scaler
    "squashing_scaler": PreprocessorConfig(
        name="squashing_scaler_default",
        categorical_name="ordinal_very_common_categories_shuffled",
        max_features_per_estimator=500,
    ),
    # Power transformations
    "safepower": PreprocessorConfig(
        name="safepower",
        categorical_name="onehot",
        max_features_per_estimator=500,
    ),
    "safepower_box": PreprocessorConfig(
        name="safepower_box",
        categorical_name="ordinal",
        max_features_per_estimator=500,
    ),
    # Robust scaler
    "robust": PreprocessorConfig(
        name="robust",
        categorical_name="numeric",
        max_features_per_estimator=500,
    ),
    # NOTE: KDI transformations are excluded due to a known bug with imputation_values_
    # "kdi_uni": PreprocessorConfig(...),
    # "kdi_alpha_1.0": PreprocessorConfig(...),
    # With append_original
    "quantile_append_original": PreprocessorConfig(
        name="quantile_uni_coarse",
        categorical_name="numeric",
        append_original=True,
        max_features_per_estimator=500,
    ),
    # With global transformer
    "none_with_svd": PreprocessorConfig(
        name="none",
        categorical_name="numeric",
        global_transformer_name="svd",
        max_features_per_estimator=500,
    ),
    "squashing_with_svd_quarter": PreprocessorConfig(
        name="squashing_scaler_default",
        categorical_name="ordinal_very_common_categories_shuffled",
        global_transformer_name="svd_quarter_components",
        max_features_per_estimator=500,
    ),
    # NOTE: scaler global transformer has a bug, excluding for now
    # "quantile_with_scaler": PreprocessorConfig(...),
}


def _build_test_cases() -> dict[str, _PipelineConsistencyCase]:
    """Build all test cases from preprocessor configs and variations."""
    test_cases: dict[str, _PipelineConsistencyCase] = {}

    # Basic preprocessor config tests
    for config_name, preprocess_config in _PREPROCESSOR_CONFIGS.items():
        test_cases[f"basic_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(_create_ensemble_config, preprocess_config),
            description=f"Basic pipeline with {config_name}",
        )

    # Test with fingerprint features
    for config_name in ["none_numeric", "quantile_uni_coarse", "squashing_scaler"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"fingerprint_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, add_fingerprint_feature=True
            ),
            description=f"Pipeline with fingerprint and {config_name}",
        )

    # Test with polynomial features
    for config_name in ["none_numeric", "quantile_uni_coarse"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"poly_all_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, polynomial_features="all"
            ),
            description=f"Pipeline with poly=all and {config_name}",
        )
        test_cases[f"poly_3_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, polynomial_features=3
            ),
            description=f"Pipeline with poly=3 and {config_name}",
        )

    # Test with feature shuffling
    for config_name in ["none_numeric", "squashing_scaler"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"shuffle_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config,
                preprocess_config,
                feature_shift_count=3,
                feature_shift_decoder="shuffle",
            ),
            description=f"Pipeline with shuffle and {config_name}",
        )
        test_cases[f"rotate_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config,
                preprocess_config,
                feature_shift_count=2,
                feature_shift_decoder="rotate",
            ),
            description=f"Pipeline with rotate and {config_name}",
        )

    # Combined test: fingerprint + polynomial + shuffle
    test_cases["combined_all_options"] = _PipelineConsistencyCase(
        config_factory=partial(
            _create_ensemble_config,
            _PREPROCESSOR_CONFIGS["quantile_uni_coarse"],
            add_fingerprint_feature=True,
            polynomial_features=3,
            feature_shift_count=2,
            feature_shift_decoder="shuffle",
        ),
        description="Pipeline with all options enabled",
    )

    return test_cases


TEST_CASES = _build_test_cases()
RANDOM_STATE = 42
ENABLED_PLATFORMS = ["darwin_arm64"]
"""The tests are enabled if _get_current_platform() returns a string in this set."""


def _get_current_platform_string() -> str:
    """Return a string identifying the current platform.

    For now we just test on MacOS, and simply identifying that seems to be enough
    to get consistent outputs.
    """
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        return "darwin_arm64"
    return "unknown"


HOW_TO_FIX_MESSAGE = (
    "If this is expected, regenerate the reference data by running: "
    "python -m tests.test_preprocessing.test_pipeline_consistency"
)
REFERENCE_DIR = (
    pathlib.Path(__file__).parent / "reference_preprocessing" / "pipeline_consistency"
)
"""The directory that contains the reference preprocessing outputs."""


def _get_platform_dir() -> pathlib.Path:
    """Return the platform-specific directory for reference data."""
    return REFERENCE_DIR / _get_current_platform_string()


def _get_reference_path(test_name: str) -> pathlib.Path:
    """Get the path to the reference file for a test case."""
    return _get_platform_dir() / f"{test_name}.json"


def _transform_with_pipeline(
    test_case: _PipelineConsistencyCase,
) -> tuple[np.ndarray, np.ndarray, dict[str, list[int]]]:
    """Run the preprocessing pipeline and return transformed data.

    Args:
        test_case: The test case configuration.

    Returns:
        Tuple of (X_train_transformed, X_test_transformed, metadata_dict).
    """
    rng = np.random.default_rng(RANDOM_STATE)
    X_train, metadata = _get_random_data_with_categoricals(rng, n_samples=30)
    X_test, _ = _get_random_data_with_categoricals(rng, n_samples=10)

    config = test_case.config_factory()
    pipeline = build_pipeline(config, random_state=RANDOM_STATE)

    # Fit and transform training data
    result_train = pipeline.fit_transform(X_train, metadata)

    # Transform test data
    result_test = pipeline.transform(X_test)

    # Convert metadata to serializable format
    metadata_dict = {
        modality.value: indices
        for modality, indices in result_train.metadata.indices_by_modality.items()
    }

    # Ensure we return numpy arrays (pipeline may return tensors in some cases)
    X_train_out = np.asarray(result_train.X)
    X_test_out = np.asarray(result_test.X)

    return X_train_out, X_test_out, metadata_dict


def _array_from_json_with_nans(data: list) -> np.ndarray:
    """Convert a JSON list back to numpy array, restoring NaN values.

    Args:
        data: Nested list from JSON where None represents NaN.

    Returns:
        Numpy float64 array with NaN values restored.
    """
    # First convert to float array, then replace None with NaN
    arr = np.array(data, dtype=object)
    # Create a float array with the same shape
    result = np.zeros(arr.shape, dtype=np.float64)
    # Handle the conversion element by element for proper NaN handling
    it = np.nditer(arr, flags=["multi_index", "refs_ok"])
    while not it.finished:
        val = it[0].item()
        result[it.multi_index] = np.nan if val is None else float(val)
        it.iternext()
    return result


def _load_reference_or_fail(
    test_name: str,
) -> tuple[np.ndarray, np.ndarray, dict[str, list[int]]]:
    """Load reference data from file or fail with helpful message.

    Args:
        test_name: Name of the test case.

    Returns:
        Tuple of (X_train_ref, X_test_ref, metadata_dict).
    """
    path = _get_reference_path(test_name)
    if not path.exists():
        raise AssertionError(f"Reference data missing at {path}\n{HOW_TO_FIX_MESSAGE}")

    with path.open("r") as f:
        data = json.load(f)

    return (
        _array_from_json_with_nans(data["X_train"]),
        _array_from_json_with_nans(data["X_test"]),
        data["metadata"],
    )


def _save_reference(
    test_name: str,
    X_train: np.ndarray,
    X_test: np.ndarray,
    metadata: dict[str, list[int]],
) -> None:
    """Save reference data to file.

    Args:
        test_name: Name of the test case.
        X_train: Transformed training data.
        X_test: Transformed test data.
        metadata: Column metadata dictionary.
    """
    path = _get_reference_path(test_name)
    path.parent.mkdir(parents=True, exist_ok=True)

    # Replace NaN with null for JSON serialization
    X_train_list = np.where(np.isnan(X_train), None, X_train).tolist()
    X_test_list = np.where(np.isnan(X_test), None, X_test).tolist()

    with path.open("w") as f:
        json.dump(
            {
                "X_train": X_train_list,
                "X_test": X_test_list,
                "metadata": metadata,
            },
            f,
            indent=2,
        )

    logger.info(f"Reference data saved for {test_name} at {path}")


@pytest.mark.skipif(
    _get_current_platform_string() not in ENABLED_PLATFORMS,
    reason="Current platform does not have consistency tests enabled.",
)
@pytest.mark.parametrize(("test_case_name", "test_case"), TEST_CASES.items())
def test__pipeline__output_matches_reference(
    test_case_name: str, test_case: _PipelineConsistencyCase
) -> None:
    """Test that pipeline output matches saved reference data."""
    X_train_ref, X_test_ref, metadata_ref = _load_reference_or_fail(test_case_name)
    X_train, X_test, metadata = _transform_with_pipeline(test_case)

    # Check shapes match
    assert X_train.shape == X_train_ref.shape, (
        f"Training data shape mismatch: {X_train.shape} vs {X_train_ref.shape}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )
    assert X_test.shape == X_test_ref.shape, (
        f"Test data shape mismatch: {X_test.shape} vs {X_test_ref.shape}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )

    # Check metadata matches
    assert metadata == metadata_ref, (
        f"Metadata mismatch:\nGot: {metadata}\nExpected: {metadata_ref}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )

    # Check values match (with tolerance for floating point)
    # Handle NaN values properly
    train_nan_mask = np.isnan(X_train_ref)
    test_nan_mask = np.isnan(X_test_ref)

    assert np.array_equal(np.isnan(X_train), train_nan_mask), (
        f"NaN positions in training data don't match.\n{HOW_TO_FIX_MESSAGE}"
    )
    assert np.array_equal(np.isnan(X_test), test_nan_mask), (
        f"NaN positions in test data don't match.\n{HOW_TO_FIX_MESSAGE}"
    )

    # Compare non-NaN values
    np.testing.assert_allclose(
        X_train[~train_nan_mask],
        X_train_ref[~train_nan_mask],
        rtol=1e-5,
        atol=1e-5,
        err_msg=f"Training data values changed.\n{HOW_TO_FIX_MESSAGE}",
    )
    np.testing.assert_allclose(
        X_test[~test_nan_mask],
        X_test_ref[~test_nan_mask],
        rtol=1e-5,
        atol=1e-5,
        err_msg=f"Test data values changed.\n{HOW_TO_FIX_MESSAGE}",
    )


def save_reference_data() -> None:
    """Generate and save reference data for all test cases."""
    _get_platform_dir().mkdir(parents=True, exist_ok=True)

    for test_case_name, test_case in TEST_CASES.items():
        try:
            X_train, X_test, metadata = _transform_with_pipeline(test_case)
            _save_reference(test_case_name, X_train, X_test, metadata)
        except Exception as e:
            logger.error(f"Failed to generate reference for {test_case_name}: {e}")
            raise


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    save_reference_data()
