{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import openml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_openml_list, test_dids_classification, valid_large_classification, open_cc_dids, open_cc_valid_dids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamer = {'name': 'Name', 'NumberOfFeatures': '# Features', 'NumberOfSymbolicFeatures': '# Categorical Features', 'NumberOfInstances': '# Instances', 'NumberOfMissingValues': '# NaNs', 'NumberOfClasses': '# Classes', 'MinorityClassSize': 'Minority Class Size'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(99,\n",
       "              {'id': 99,\n",
       "               'alias': 'OpenML-CC18',\n",
       "               'main_entity_type': 'task',\n",
       "               'name': 'OpenML-CC18 Curated Classification benchmark',\n",
       "               'status': 'active',\n",
       "               'creation_date': '2019-02-21 18:47:13',\n",
       "               'creator': 1}),\n",
       "             (225,\n",
       "              {'id': 225,\n",
       "               'alias': 'OpenML-friendly',\n",
       "               'main_entity_type': 'task',\n",
       "               'name': 'OpenML100-friendly',\n",
       "               'status': 'active',\n",
       "               'creation_date': '2019-09-16 19:41:46',\n",
       "               'creator': 1})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml.study.list_suites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = openml.study.get_suite(suite_id=99)\n",
    "tasks = openml.tasks.list_tasks(output_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ``@`` in `pd.DataFrame.query <\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html>`_\n",
    "# accesses variables outside of the current dataframe.\n",
    "tasks = tasks.query(\"tid in @suite.tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids = list(tasks[np.logical_and(np.logical_and((tasks.NumberOfInstances <= 2000), (tasks.NumberOfFeatures <= 100))\n",
    "                                 , (tasks.NumberOfClasses <= 10))].tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids = list(tasks[tasks.NumberOfInstances <= 2000].tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_cc_dids = [openml.tasks.get_task(task_id).get_dataset().id for task_id in tids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "open_ml_datasets, open_ml_datasets_df = load_openml_list(test_dids_classification, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 100000, num_feats=100, return_capped=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ml_datasets_df = open_ml_datasets_df[open_ml_datasets_df.NumberOfInstances > 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "                                  Name &  \\# Features &  \\# Categorical Features &  \\# Instances &  \\# Classes &  \\# NaNs &  Minority Class Size &    id \\\\\n",
      "\\midrule\n",
      "                    KDDCup09\\_appetency &         231 &                      39 &        50000 &          2 & 8024152 &                  890 &  1111 \\\\\n",
      "                              airlines &           8 &                       5 &       539383 &          2 &       0 &               240264 &  1169 \\\\\n",
      "                        bank-marketing &          17 &                      10 &        45211 &          2 &       0 &                 5289 &  1461 \\\\\n",
      "                                 nomao &         119 &                      30 &        34465 &          2 &       0 &                 9844 &  1486 \\\\\n",
      "                                 adult &          15 &                       9 &        48842 &          2 &    6465 &                11687 &  1590 \\\\\n",
      "                             covertype &          55 &                      45 &       581012 &          7 &       0 &                 2747 &  1596 \\\\\n",
      "                           numerai28.6 &          22 &                       1 &        96320 &          2 &       0 &                47662 & 23517 \\\\\n",
      "                             connect-4 &          43 &                      43 &        67557 &          3 &       0 &                 6449 & 40668 \\\\\n",
      "jungle\\_chess\\_2pcs\\_raw\\_endgame\\_complete &           7 &                       1 &        44819 &          3 &       0 &                 4335 & 41027 \\\\\n",
      "                            APSFailure &         171 &                       1 &        76000 &          2 & 1078695 &                 1375 & 41138 \\\\\n",
      "                                albert &          79 &                      53 &       425240 &          2 & 2734000 &               212620 & 41147 \\\\\n",
      "                             MiniBooNE &          51 &                       1 &       130064 &          2 &       0 &                36499 & 41150 \\\\\n",
      "                             guillermo &        4297 &                       1 &        20000 &          2 &       0 &                 8003 & 41159 \\\\\n",
      "                              riccardo &        4297 &                       1 &        20000 &          2 &       0 &                 5000 & 41161 \\\\\n",
      "                               volkert &         181 &                       1 &        58310 &         10 &       0 &                 1361 & 41166 \\\\\n",
      "                                dionis &          61 &                       1 &       416188 &        355 &       0 &                  878 & 41167 \\\\\n",
      "                                jannis &          55 &                       1 &        83733 &          4 &       0 &                 1687 & 41168 \\\\\n",
      "                                helena &          28 &                       1 &        65196 &        100 &       0 &                  111 & 41169 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_table = open_ml_datasets_df\n",
    "print_table = print_table[['name', 'NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']].copy()\n",
    "print_table['id'] = print_table.index\n",
    "print_table[['NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']] = print_table[['NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']].astype(int)\n",
    "print_table = print_table.rename(columns=renamer)\n",
    "print(print_table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 11 from '/home/hollmann/.cache/openml/org/openml/www/datasets/11/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 12 from '/home/hollmann/.cache/openml/org/openml/www/datasets/12/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 35\n",
      "Loading balance-scale 11 ..\n",
      "Loading mfeat-factors 12 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 14 from '/home/hollmann/.cache/openml/org/openml/www/datasets/14/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mfeat-fourier 14 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 15 from '/home/hollmann/.cache/openml/org/openml/www/datasets/15/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 16 from '/home/hollmann/.cache/openml/org/openml/www/datasets/16/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading breast-w 15 ..\n",
      "Loading mfeat-karhunen 16 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 18 from '/home/hollmann/.cache/openml/org/openml/www/datasets/18/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 22 from '/home/hollmann/.cache/openml/org/openml/www/datasets/22/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mfeat-morphological 18 ..\n",
      "Loading mfeat-zernike 22 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 23 from '/home/hollmann/.cache/openml/org/openml/www/datasets/23/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 29 from '/home/hollmann/.cache/openml/org/openml/www/datasets/29/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cmc 23 ..\n",
      "Loading credit-approval 29 ..\n",
      "Loading credit-g 31 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 31 from '/home/hollmann/.cache/openml/org/openml/www/datasets/31/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 37 from '/home/hollmann/.cache/openml/org/openml/www/datasets/37/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 50 from '/home/hollmann/.cache/openml/org/openml/www/datasets/50/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 54 from '/home/hollmann/.cache/openml/org/openml/www/datasets/54/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diabetes 37 ..\n",
      "Loading tic-tac-toe 50 ..\n",
      "Loading vehicle 54 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 188 from '/home/hollmann/.cache/openml/org/openml/www/datasets/188/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 307 from '/home/hollmann/.cache/openml/org/openml/www/datasets/307/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eucalyptus 188 ..\n",
      "Loading vowel 307 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 458 from '/home/hollmann/.cache/openml/org/openml/www/datasets/458/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading analcatdata_authorship 458 ..\n",
      "Loading analcatdata_dmft 469 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 469 from '/home/hollmann/.cache/openml/org/openml/www/datasets/469/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1049 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1049/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1050 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1050/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pc4 1049 ..\n",
      "Loading pc3 1050 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1063 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1063/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1068 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1068/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading kc2 1063 ..\n",
      "Loading pc1 1068 ..\n",
      "Loading banknote-authentication 1462 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1462 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1462/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1464 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1464/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1468 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1468/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading blood-transfusion-service-center 1464 ..\n",
      "Loading cnae-9 1468 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1480 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1480/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1494 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1494/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ilpd 1480 ..\n",
      "Loading qsar-biodeg 1494 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1501 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1501/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semeion 1501 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 1510 from '/home/hollmann/.cache/openml/org/openml/www/datasets/1510/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 6332 from '/home/hollmann/.cache/openml/org/openml/www/datasets/6332/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wdbc 1510 ..\n",
      "Loading cylinder-bands 6332 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 23381 from '/home/hollmann/.cache/openml/org/openml/www/datasets/23381/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n",
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 40966 from '/home/hollmann/.cache/openml/org/openml/www/datasets/40966/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dresses-sales 23381 ..\n",
      "Loading MiceProtein 40966 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 40975 from '/home/hollmann/.cache/openml/org/openml/www/datasets/40975/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading car 40975 ..\n",
      "Loading mfeat-pixel 40979 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 40982 from '/home/hollmann/.cache/openml/org/openml/www/datasets/40982/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading steel-plates-fault 40982 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Encountered unsupported pickle protocol when loading dataset 40994 from '/home/hollmann/.cache/openml/org/openml/www/datasets/40994/dataset.pkl.py3'. Error message was: unsupported pickle protocol: 5. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading climate-model-simulation-crashes 40994 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [00:17<00:00, 63.10it/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_datasets_multiclass_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_2567152/487285290.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;31m# Filter out datasets in Open CC\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m \u001B[0mopenml_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mopenml_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'CFI'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfClasses\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfFeatures\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfInstances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'CFI'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfClasses\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfFeatures\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfInstances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/TabPFN/lib/python3.7/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4355\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfloat64\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4356\u001B[0m         \"\"\"\n\u001B[0;32m-> 4357\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mSeriesApply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconvert_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4358\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4359\u001B[0m     def _reduce(\n",
      "\u001B[0;32m~/anaconda3/envs/TabPFN/lib/python3.7/site-packages/pandas/core/apply.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1041\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_str\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1043\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1044\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1045\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/TabPFN/lib/python3.7/site-packages/pandas/core/apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1099\u001B[0m                     \u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m                     \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# type: ignore[arg-type]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1101\u001B[0;31m                     \u001B[0mconvert\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_dtype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1102\u001B[0m                 )\n\u001B[1;32m   1103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/TabPFN/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_2567152/487285290.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;31m# Filter out datasets in Open CC\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m \u001B[0mopenml_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mopenml_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'CFI'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopenml_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfClasses\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfFeatures\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfInstances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'CFI'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_datasets_multiclass_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfClasses\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfFeatures\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumberOfInstances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test_datasets_multiclass_df' is not defined"
     ]
    }
   ],
   "source": [
    "open_cc_datasets, open_cc_datasets_df = load_openml_list(open_cc_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 2000, num_feats=100, return_capped=True)\n",
    "\n",
    "def extend_datasets(datasets, filtering = False):\n",
    "    extended_datasets = {}\n",
    "    i = 0\n",
    "    for d in tqdm(datasets):\n",
    "        if ((not 'NumberOfFeatures' in datasets[d])\n",
    "                or (not 'NumberOfClasses' in datasets[d])\n",
    "                or (not 'NumberOfInstances' in datasets[d])\n",
    "                # or datasets[d]['NumberOfFeatures'] >= num_feats\n",
    "                or datasets[d]['NumberOfClasses'] <= 0):\n",
    "            print(datasets[d])\n",
    "            continue\n",
    "        ds = openml.datasets.get_dataset(d, download_data=False)\n",
    "        if filtering and (datasets[d]['NumberOfInstances'] < 150\n",
    "                          or datasets[d]['NumberOfInstances'] > 2000\n",
    "                         or datasets[d]['NumberOfFeatures'] > 100\n",
    "                         or datasets[d]['NumberOfClasses'] > 10):\n",
    "            continue\n",
    "        extended_datasets[d] = datasets[d]\n",
    "        extended_datasets[d].update(ds.qualities)\n",
    "    \n",
    "    return extended_datasets\n",
    "\n",
    "# All datasets\n",
    "openml_list = openml.datasets.list_datasets()\n",
    "openml_list = pd.DataFrame.from_dict(openml_list, orient=\"index\")\n",
    "\n",
    "# Select only classification\n",
    "openml_list = openml_list[~openml_list['MajorityClassSize'].isna()]\n",
    "\n",
    "# Remove duplicated datasets\n",
    "duplicated = openml_list.duplicated(subset=['MajorityClassSize', 'MaxNominalAttDistinctValues', 'MinorityClassSize',\n",
    "       'NumberOfClasses', 'NumberOfFeatures', 'NumberOfInstances',\n",
    "       'NumberOfInstancesWithMissingValues', 'NumberOfMissingValues',\n",
    "       'NumberOfNumericFeatures', 'NumberOfSymbolicFeatures'], keep='first')\n",
    "openml_list = openml_list[~duplicated]\n",
    "\n",
    "duplicated = openml_list.duplicated(subset=['name'], keep='first')\n",
    "openml_list = openml_list[~duplicated]\n",
    "\n",
    "# Filter out datasets that don't have meta information or Don't fulfill other criteria\n",
    "openml_list = openml_list.to_dict(orient='index')\n",
    "openml_list = pd.DataFrame.from_dict(extend_datasets(openml_list, filtering=True), orient=\"index\")\n",
    "\n",
    "# Filter out datasets in Open CC\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: x in test_datasets_multiclass_df.name.values)]\n",
    "openml_list['CFI'] = openml_list.apply(lambda x: str(x.NumberOfClasses) + '_' + str(x.NumberOfFeatures) + '_' + str(x.NumberOfInstances), axis = 1)\n",
    "test_datasets_multiclass_df['CFI'] = test_datasets_multiclass_df.apply(lambda x: str(x.NumberOfClasses) + '_' + str(x.NumberOfFeatures) + '_' + str(x.NumberOfInstances), axis = 1)\n",
    "openml_list = openml_list[~openml_list.CFI.apply(lambda x: x in test_datasets_multiclass_df.CFI.values)]\n",
    "\n",
    "# Remove time series and artificial data\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'autoUniv' in x)]\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'fri_' in x)]\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'FOREX' in x)]\n",
    "\n",
    "# Remove datasets that overlapped with Open CC closely by name\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'ilpd' in x)]\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'car' in x)]\n",
    "openml_list = openml_list[~openml_list.name.apply(lambda x: 'pc1' in x)]\n",
    "\n",
    "# Remove datasets that didn't load\n",
    "openml_list = openml_list[~openml_list.did.apply(lambda x: x in {1065, 40589, 41496, 770, 43097, 43148, 43255, 43595, 43786, 41701})]\n",
    "\n",
    "# Remove class skew\n",
    "openml_list = openml_list[(openml_list.MinorityClassSize / openml_list.MajorityClassSize) > 0.05]\n",
    "openml_list = openml_list[openml_list.AutoCorrelation != 1]\n",
    "\n",
    "# Remove too easy\n",
    "openml_list = openml_list[openml_list.CfsSubsetEval_DecisionStumpAUC != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table = openml_list\n",
    "print_table = print_table[['name', 'NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']].copy()\n",
    "print_table['id'] = print_table.index\n",
    "print_table[['NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']] = print_table[['NumberOfFeatures', 'NumberOfSymbolicFeatures', 'NumberOfInstances', 'NumberOfClasses', 'NumberOfMissingValues', 'MinorityClassSize']].astype(int)\n",
    "print_table = print_table.rename(columns=renamer)\n",
    "print(print_table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}